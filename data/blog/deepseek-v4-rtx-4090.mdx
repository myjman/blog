---
title: 'DeepSeek V4가 RTX 4090 두 장으로 돌아가는 이유'
date: '2026-02-17T18:00:13'
tags:
  [
    'deepseek',
    '딥시크',
    'deepseek-v4',
    '딥시크V4',
    'rtx-4090',
    'consumer-gpu',
    '소비자GPU',
    'moe',
    'mixture-of-experts',
    'mla',
    'ai-coding',
    'AI코딩',
    'open-source-ai',
    '오픈소스AI',
  ]
draft: false
images: ['/static/images/deepseek-v4-rtx-4090/og-image.jpg']
summary: 'DeepSeek V4는 1조 파라미터 모델이지만 RTX 4090 두 장으로 구동된다. MoE 아키텍처와 MLA 압축 기술이 만들어낸 결과다. 5만 달러짜리 H100 인프라 없이도 GPT-4급 추론이 가능해졌다.'
---

## 5만 달러짜리 인프라가 150만 원으로 줄었다

![RTX 4090 그래픽카드 — 이제 1조 파라미터 모델을 돌린다](/static/images/deepseek-v4-rtx-4090/og-image.jpg)

2026년 2월 17일. 음력 설날이다. DeepSeek이 V4를 공개할 것으로 예상되는 날이기도 하다.

이 모델이 주목받는 이유는 단순하다. **1조 개의 파라미터**를 가진 거대 모델인데, **RTX 4090 두 장**으로 돌아간다. 4090 두 장이면 약 150만 원이다. 같은 성능을 내려면 과거에는 NVIDIA H100 클러스터가 필요했다. 최소 **5만 달러** 이상의 인프라 비용이다.

비용이 **97% 이상** 줄었다. 어떻게 가능한 걸까.

세 가지 기술이 핵심이다. MoE(Mixture of Experts) 아키텍처, MLA(Multi-head Latent Attention) 압축, 그리고 DeepSeek Sparse Attention이다. 이 세 가지가 조합되어 "1조 파라미터지만 실제로는 320억만 사용한다"는 마법 같은 효율을 만들어냈다.

GPT-4를 로컬에서 돌린다는 것. 1년 전만 해도 망상이었다. 이제 현실이 됐다.

---

## MoE: 1조 파라미터 중 3%만 깨운다

![데이터센터 서버룸 — 과거의 AI 추론 인프라](/static/images/deepseek-v4-rtx-4090/data-center.jpg)

MoE는 Mixture of Experts의 약자다. 직역하면 "전문가들의 혼합"이다. 핵심 아이디어는 간단하다. **모든 파라미터를 동시에 사용하지 않는다.**

DeepSeek V4는 총 1조 개(1 trillion)의 파라미터를 가진다. 하지만 하나의 토큰을 처리할 때 실제로 활성화되는 파라미터는 **약 320억 개**(32B)뿐이다. 전체의 **3% 정도**다. 나머지 97%는 잠들어 있다.

어떻게 작동하는가. 모델 내부에 수백 개의 "전문가 네트워크"가 있다. 각 전문가는 특정 유형의 패턴을 처리하는 데 특화되어 있다. 입력이 들어오면, "라우터" 모듈이 해당 입력에 가장 적합한 전문가 몇 명만 선택해서 활성화한다.

비유하자면 대형 병원과 같다. 병원에 100명의 전문의가 있다. 하지만 환자 한 명을 진료할 때 100명 전원이 달려드는 건 아니다. 증상에 맞는 2~3명의 전문의만 담당한다. 나머지는 다른 환자를 보거나 대기한다.

MoE의 장점은 **파라미터 수와 연산량을 분리**한다는 것이다. 1조 파라미터의 "지식 용량"을 가지면서도, 실제 연산은 320억 파라미터 모델 수준으로 유지한다. 메모리에는 전체 파라미터가 올라가야 하지만, 연산 비용은 훨씬 적다.

DeepSeek V3(V4의 전신)의 기술 보고서에 따르면, MoE 구조 덕분에 동일 성능 대비 학습 비용을 **약 5배** 줄였다. V4는 이 효율을 더 극대화했다.

| 모델         | 총 파라미터 | 활성 파라미터 | 활성화 비율 |
| ------------ | ----------- | ------------- | ----------- |
| GPT-4 (추정) | 1.8조       | 1.8조         | 100%        |
| DeepSeek V4  | 1조         | 320억         | ~3%         |
| DeepSeek V3  | 6,710억     | 370억         | ~5%         |
| Llama 3 70B  | 700억       | 700억         | 100%        |

GPT-4와 DeepSeek V4를 비교하면 흥미롭다. 두 모델 모두 조 단위 파라미터를 가지지만, GPT-4는 모든 파라미터가 항상 활성화된다(Dense 모델). DeepSeek V4는 3%만 활성화된다(Sparse 모델). 추론 비용 차이가 **20~50배**에 달하는 이유다.

---

## MLA: KV 캐시를 93.3% 압축한다

MoE가 연산량을 줄였다면, MLA(Multi-head Latent Attention)는 **메모리 사용량**을 줄인다.

트랜스포머 모델에서 가장 큰 메모리 병목은 **KV 캐시**(Key-Value Cache)다. 모델이 이전 토큰들을 "기억"하기 위해 저장하는 데이터다. 맥락 창이 길어질수록 KV 캐시도 선형적으로 증가한다. 100만 토큰 맥락 창이면 KV 캐시만 수십 GB가 필요하다.

MLA의 핵심 아이디어는 **저차원 압축**(low-rank compression)이다. 원래의 Key와 Value를 그대로 저장하지 않고, 먼저 더 작은 차원의 "잠재 벡터"(latent vector)로 압축한다. 캐시에는 이 압축된 벡터만 저장한다. 사용할 때 다시 복원한다.

수학적으로 보면 이렇다. 기존 방식에서는 입력 X에 가중치 행렬 W_KV를 곱해 Key와 Value를 만든다. MLA에서는 W_KV를 **두 개의 작은 행렬로 분해**한다. W_DKV(압축 행렬)와 W_UK, W_UV(복원 행렬)다.

```
기존: X → W_KV → K, V (큰 벡터, 캐시에 저장)
MLA:  X → W_DKV → C_KV (작은 벡터, 캐시에 저장) → W_UK, W_UV → K, V
```

DeepSeek V2 논문에서 보고된 결과가 놀랍다. MLA를 적용하면 KV 캐시 크기가 **93.3% 감소**한다. 원래 100GB가 필요하던 KV 캐시가 6.7GB로 줄어든다.

더 놀라운 점은 성능 저하가 거의 없다는 것이다. 오히려 MLA가 기존 Multi-head Attention(MHA)보다 **약간 더 좋은** 모델링 성능을 보인다고 보고됐다. 압축이 일종의 정규화(regularization) 효과를 내기 때문으로 해석된다.

구체적인 구현 수치를 보자. DeepSeek V4에서 어텐션 헤드 수(n_h)는 128개, 헤드당 차원(d_h)은 128이다. KV 압축 차원(d_c)은 512다. 원래 차원(128 × 128 = 16,384)이 512로 줄어드니, 압축률은 약 32:1이다.

이 압축 덕분에 **RTX 4090 한 장**(24GB VRAM)에서도 DeepSeek V4를 돌릴 수 있게 됐다. 물론 풀 성능을 내려면 두 장이 필요하지만, 기본적인 추론은 단일 GPU로도 가능하다.

---

## DeepSeek Sparse Attention: 100만 토큰을 한 번에

![회로 기판 — AI 칩 기술의 발전](/static/images/deepseek-v4-rtx-4090/circuit-board.jpg)

MoE와 MLA가 "모델 내부"의 효율을 높였다면, DeepSeek Sparse Attention은 "긴 맥락 처리"의 효율을 높인다.

DeepSeek V4의 맥락 창은 **100만 토큰 이상**이다. 일반적인 코드베이스 전체를 한 번에 읽을 수 있는 크기다. 하지만 100만 토큰에 대해 풀 어텐션을 계산하면 연산량이 **토큰 수의 제곱**에 비례해서 증가한다. 100만 토큰이면 1조 번의 어텐션 연산이 필요하다.

DeepSeek Sparse Attention은 **동적 희소성**(dynamic sparsity)을 사용한다. 모든 토큰 쌍에 대해 어텐션을 계산하지 않고, "관련성이 높은" 토큰 쌍만 선택적으로 계산한다.

작동 방식은 이렇다. "Lightning Indexer"라는 모듈이 입력을 분석한다. 각 쿼리에 대해 맥락 내에서 가장 관련성 높은 키(key)들을 빠르게 식별한다. 상위 K개의 관련 블록에 대해서만 어텐션을 계산한다.

비유하자면 도서관에서 책을 찾는 것과 같다. 100만 권의 책이 있는 도서관에서 모든 책을 훑어보는 대신, 색인 시스템을 사용해 관련 있는 책만 빠르게 찾아낸다.

이 접근법 덕분에 100만 토큰 맥락에서도 **연산량 증가가 선형적**이다. 토큰 수가 10배 늘어도 연산량은 10배만 늘어난다. 제곱으로 폭발하지 않는다.

실질적인 효과는 이렇다. 기존 Dense Attention 모델에서는 100만 토큰 처리가 사실상 불가능했다. 메모리도 부족하고 시간도 너무 오래 걸린다. DeepSeek V4는 100만 토큰을 **실시간에 가깝게** 처리한다.

---

## Engram 메모리: O(1) 시간에 지식을 찾는다

MoE, MLA, Sparse Attention 외에 DeepSeek V4에는 또 하나의 혁신이 있다. **Engram 조건부 메모리** 시스템이다.

기존 트랜스포머는 "정적 패턴 저장"과 "동적 추론"이 뒤섞여 있다. 모델이 학습한 지식(예: 문법 규칙, 상식)과 실시간 추론(예: 현재 맥락에서의 논리적 판단)이 같은 메커니즘으로 처리된다.

Engram은 이 둘을 **분리**한다. 자주 사용되는 정적 패턴은 별도의 메모리 모듈에 저장한다. 이 메모리는 **O(1) 시간 복잡도**로 조회된다. 입력이 들어오면 먼저 Engram 메모리에서 관련 패턴을 빠르게 가져온 다음, 그 위에서 동적 추론을 수행한다.

개념적으로는 N-gram 임베딩의 현대화 버전이다. 고전적인 N-gram 모델은 빠르지만 표현력이 제한적이었다. Engram은 신경망의 표현력을 유지하면서 N-gram의 조회 속도를 가져온다.

실제 효과는 **첫 토큰 생성 시간**(time to first token) 단축으로 나타난다. 긴 프롬프트를 처리할 때, 기존 모델은 프롬프트 전체를 순차적으로 인코딩해야 한다. Engram이 있으면 자주 등장하는 패턴은 즉시 조회된다. 인코딩 시간이 줄어든다.

---

## 벤치마크: SWE-bench 80% 돌파를 노린다

기술은 화려하다. 그런데 성능은 어떤가.

DeepSeek V4의 공식 벤치마크는 아직 공개되지 않았다(2026년 2월 17일 현재). 하지만 내부 테스트 결과가 여러 경로로 유출됐다.

가장 주목받는 건 **SWE-bench** 성능이다. SWE-bench는 실제 GitHub 이슈를 해결하는 능력을 측정하는 벤치마크다. 현재 최고 기록은 Claude Opus 4.5의 **80.9%**다.

DeepSeek 내부 테스트에서 V4는 **80% 이상**을 기록했다는 소문이 돈다. 사실이라면 Claude Opus 4.5와 동급이다. 그런데 비용은 **20~50배 저렴**하다.

| 모델            | SWE-bench Verified | 추론 비용 (1M 토큰) |
| --------------- | ------------------ | ------------------- |
| Claude Opus 4.5 | 80.9%              | ~$75 (출력)         |
| GPT-4o          | ~60%               | ~$30 (출력)         |
| DeepSeek V4     | ~80% (미확인)      | ~$0.42 (출력)       |
| DeepSeek V3.2   | ~65%               | ~$0.42 (출력)       |

SWE-bench 외에도 여러 코딩 벤치마크에서 강세가 예상된다. DeepSeek V3는 이미 **LiveCodeBench**에서 최고 성능을 기록했다. 수학 벤치마크 **MATH-500**에서는 90.2%를 찍어 Claude(78.3%)를 크게 앞섰다.

V4는 V3 대비 아키텍처 개선이 상당하다. mHC(Manifold-Constrained Hyper-Connections) 기법이 추가됐다. 이 기법은 깊은 네트워크에서 발생하는 신호 폭발 문제를 해결한다. 기존 방식에서는 레이어가 깊어질수록 활성화 값이 3,000배까지 폭발했다. mHC는 이를 **1.6배**로 억제한다.

신호 폭발 억제 덕분에 더 깊은 네트워크를 안정적으로 학습할 수 있다. 더 깊은 네트워크는 더 복잡한 패턴을 학습할 수 있다. 성능 향상의 핵심 동력이다.

---

## 가격: GPT-4 대비 1/50

![게이밍 셋업 — 이제 AI 추론 인프라가 된다](/static/images/deepseek-v4-rtx-4090/gaming-setup.jpg)

DeepSeek V4의 가장 큰 무기는 **가격**이다.

API 가격을 보자. DeepSeek V4는 캐시 미스 기준 입력 **$0.28/1M 토큰**, 출력 **$0.42/1M 토큰**으로 예상된다(V3.2-Exp 기준). 캐시 히트 시 입력은 **$0.028/1M 토큰**까지 떨어진다.

비교하면 이렇다.

| 모델            | 입력 ($/1M) | 출력 ($/1M) | 비율 (GPT-4 대비) |
| --------------- | ----------- | ----------- | ----------------- |
| GPT-4 Turbo     | $10         | $30         | 1x                |
| Claude Opus 4.5 | $15         | $75         | 1.5x (비쌈)       |
| DeepSeek V4     | $0.28       | $0.42       | **1/50x**         |

GPT-4 대비 **20~50배 저렴**하다. Claude Opus 대비로는 **100배 이상** 차이가 난다.

로컬 배포 비용도 극적으로 낮아졌다. RTX 4090 두 장이면 풀 성능으로 V4를 돌릴 수 있다. 4090 한 장 시세가 약 75만 원이니 총 150만 원. H100 한 장이 3~4만 달러인 것과 비교하면 **1/30 가격**이다.

더 극단적인 최적화도 가능하다. 4비트 양자화를 적용하면 **RTX 3060**(12GB VRAM)에서도 제한적으로 추론이 가능하다는 보고가 있다. 3060은 약 30만 원이다.

비용 절감의 의미는 단순히 "더 싸다"가 아니다. **로컬 배포가 현실화**된다는 것이다.

기업 입장에서 생각해 보자. 민감한 코드를 외부 API로 보내는 건 보안 위험이 있다. 로컬에서 돌리면 데이터가 외부로 나가지 않는다. 하지만 과거에는 로컬 배포 비용이 너무 높았다. H100 클러스터를 구축하려면 수억 원이 필요했다.

DeepSeek V4는 이 방정식을 바꾼다. **150만 원 투자**로 GPT-4급 모델을 사내에서 운영할 수 있다. 중소기업도 접근 가능한 수준이다.

개인 개발자에게도 의미가 크다. 월 100달러 API 비용을 내지 않아도 된다. 한 번 GPU를 사면 무제한으로 추론할 수 있다. 취미 프로젝트나 학습 용도로 GPT-4급 모델을 마음껏 실험할 수 있다.

---

## 오픈소스: Apache 2.0으로 공개

DeepSeek V4의 또 다른 강점은 **오픈소스**라는 것이다.

DeepSeek은 V3부터 모델 가중치를 **Apache 2.0 라이선스**로 공개해왔다. V4도 같은 라이선스로 공개될 예정이다. Apache 2.0은 상업적 사용을 포함한 거의 모든 용도를 허용하는 관대한 라이선스다.

이게 왜 중요한가.

**첫째, 수정이 자유롭다.** 특정 도메인에 맞게 파인튜닝할 수 있다. 법률 문서 분석에 특화된 버전, 의료 기록 처리에 특화된 버전을 만들 수 있다. 클로즈드 모델 API로는 이런 커스터마이징이 불가능하다.

**둘째, 검증이 가능하다.** 모델이 어떻게 작동하는지 직접 들여다볼 수 있다. 보안에 민감한 환경에서는 "블랙박스" 모델을 신뢰하기 어렵다. 오픈소스 모델은 코드 수준에서 감사가 가능하다.

**셋째, 커뮤니티 생태계가 형성된다.** V3 공개 이후 수많은 파생 프로젝트가 등장했다. 양자화 최적화 버전, 특정 언어에 특화된 버전, 에지 디바이스용 경량화 버전 등이다. V4도 같은 생태계 효과가 예상된다.

반면 GPT-4, Claude는 완전한 클로즈드 모델이다. API로만 접근 가능하다. 내부 구조는 공개되지 않는다. OpenAI와 Anthropic이 서비스를 중단하면 사용할 수 없다.

오픈소스 vs 클로즈드 논쟁에서 DeepSeek V4는 "오픈소스도 최고 성능을 낼 수 있다"는 증거가 된다. 과거에는 오픈소스 모델이 항상 클로즈드 모델에 뒤처졌다. Llama가 GPT-3.5 수준에 머물렀던 것처럼. V4가 정말로 GPT-4/Claude급 성능을 낸다면, 이 공식이 깨진다.

---

## 중국 AI의 역습

DeepSeek V4의 등장은 더 큰 맥락에서 봐야 한다. **중국 AI의 급부상**이다.

2022년 미국은 중국에 대한 반도체 수출 규제를 강화했다. H100을 포함한 고성능 GPU 수출이 금지됐다. 중국 AI 기업들은 하드웨어 접근이 제한됐다.

많은 전문가가 예측했다. "중국 AI는 뒤처질 것이다." 최신 하드웨어 없이는 대규모 모델 학습이 불가능하니까.

DeepSeek이 이 예측을 뒤집었다.

DeepSeek V3 학습에 사용된 비용은 **약 550만 달러**로 추정된다. GPT-4 학습 비용(수억 달러 추정)의 1/50 수준이다. H100 대신 A100을 사용했고, 알고리즘 효율로 하드웨어 열세를 상쇄했다.

V4는 더 나아간다. "최고 성능"과 "최저 비용"을 동시에 잡으려 한다. 학습 효율뿐 아니라 추론 효율까지 극대화했다.

이 전략의 의미는 이렇다. 미국 기업들은 "더 큰 모델, 더 많은 컴퓨팅"으로 성능을 높여왔다. OpenAI의 GPT-5 추론 비용이 GPT-4보다 더 비싸다는 건 공공연한 비밀이다. 스케일업 전략이다.

DeepSeek은 반대 방향으로 간다. "같은 성능, 더 적은 자원." 효율화 전략이다. 하드웨어 제약이 오히려 알고리즘 혁신을 강제했다.

누가 이길지는 아직 모른다. 하지만 한 가지는 확실하다. **경쟁이 치열해진다.** 소비자에게는 좋은 일이다. 가격이 내려가고 선택지가 늘어난다.

미국 기술주 시장도 긴장하고 있다. DeepSeek V4 출시 소식이 전해질 때마다 **NVIDIA 주가가 흔들린다.** 고가 GPU 수요가 줄어들 수 있다는 우려 때문이다. 2026년 2월 초 DeepSeek 관련 뉴스에 NVIDIA는 하루 만에 5% 이상 하락했다.

---

## 결론: 민주화인가, 군비 경쟁인가

DeepSeek V4가 RTX 4090 두 장으로 돌아가는 이유를 정리하면 이렇다.

**MoE 아키텍처**가 1조 파라미터 중 3%만 활성화한다. 연산량이 30배 줄어든다. **MLA 압축**이 KV 캐시를 93.3% 줄인다. 메모리 사용량이 15배 줄어든다. **Sparse Attention**이 100만 토큰을 선형 시간에 처리한다. 긴 맥락에서도 비용이 폭발하지 않는다.

세 기술의 조합으로, 과거 5만 달러짜리 인프라가 150만 원으로 줄었다.

이게 "AI 민주화"인지, 새로운 "군비 경쟁"의 시작인지는 관점에 따라 다르다.

민주화 관점에서 보면, 개인과 중소기업도 GPT-4급 AI에 접근할 수 있게 됐다. 대기업과 스타트업의 기술 격차가 줄어든다. 더 많은 혁신이 일어날 수 있는 환경이다.

군비 경쟁 관점에서 보면, 바닥이 없다. DeepSeek V4가 150만 원이면, 다음 세대는 50만 원이 될 수 있다. 그다음은 10만 원. "누구나 GPT-4를 돌릴 수 있는 세상"이 유토피아인지 디스토피아인지는 아무도 모른다.

확실한 건 이것이다. **AI 비용 곡선이 급락하고 있다.** 1년 전 불가능했던 게 오늘 가능해졌다. 1년 후에는 또 다른 불가능이 가능해질 것이다.

DeepSeek V4는 그 곡선 위의 하나의 점이다. 하지만 중요한 점이다. "1조 파라미터 모델이 소비자 GPU에서 돌아간다"는 이정표. 이 이정표를 지나면, 되돌아갈 수 없다.

---

**출처:**

- [DeepSeek V4 is coming this month. Why it could rattle tech stocks and Nvidia — The Motley Fool](https://www.fool.com/investing/2026/02/11/deepseek-v4-is-coming-this-month-why-it-could-ratt/)
- [DeepSeek V4: 1M+ Context, Consumer GPU, Open Weights — TamilTech](https://tamiltech.in/article/deepseek-v4-launch-february-2026-context-consumer-gpu-open-weights)
- [DeepSeek V4: Revolutionary AI Coding Model Launching February 2026 — Gaga.art](https://gaga.art/blog/deepseek-v4/)
- [DeepSeek V4's 1-Trillion Parameter Architecture Targets Western AI Coding Dominance — Introl](https://introl.com/blog/deepseek-v4-trillion-parameter-coding-model-february-2026)
- [DeepSeek V4 vs Claude Opus 4.5 for Coding: Benchmark Comparison — WaveSpeedAI](https://wavespeed.ai/blog/posts/blog-deepseek-v4-vs-claude-opus-coding/)
- [DeepSeek-V3 Explained: Multi-head Latent Attention — Towards Data Science](https://towardsdatascience.com/deepseek-v3-explained-1-multi-head-latent-attention-ed6bee2a67c4/)
- [Decoding Multi-Head Latent Attention — Vizuara](https://vizuara.substack.com/p/decoding-multi-head-latent-attention)
- [DeepSeek V4 Pricing: Why It Costs 20-50x Less Than OpenAI — Macaron](https://macaron.im/blog/deepseek-v4-pricing)
- [DeepSeek V4: Everything We Know About the Upcoming Coding AI Model — WaveSpeedAI](https://wavespeed.ai/blog/posts/deepseek-v4-everything-we-know-about-the-upcoming-coding-ai-model/)
- [Unsplash](https://unsplash.com)
